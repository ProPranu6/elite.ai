# -*- coding: utf-8 -*-
"""Elite.ai-Content-Summarisations

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WsKL9o2dZd7dS3eWUxkzwZzz8dYgOwgY

## Modules
"""

import elite_ai_ml_bckend
from elite_ai_ml_bckend import *

import tensorflow as tf
import tensorflow_hub as hub
print("TF version: ", tf.__version__)
print("Hub version: ", hub.__version__)
import tensorflow_hub as hub
import tensorflow as tf
import bert
FullTokenizer = bert.bert_tokenization.FullTokenizer
from tensorflow.keras.models import Model       # Keras is the new high level API for TensorFlow
import math
import re
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from math import *
import time
from sumy.parsers.plaintext import PlaintextParser
from sumy.summarizers.lex_rank import LexRankSummarizer
from sumy.nlp.tokenizers import Tokenizer 
from elite_ai_ml_bckend.alternative_summarizations import *
import os
import nltk
import time
nltk.download('punkt')
nltk.download('stopwords')
from nltk.corpus import stopwords

"""# Classes"""

class SentenceEncoders():
  
  list_models = ['paraphrase-mpnet-base-v2', 'distilbert-base-nli-mean-tokens']
  def __init__(self, model_name=""):
    self.model_name = model_name
    self.encoder = SentenceTransformer(model_name)
    

  @staticmethod
  def text_to_sens(transcript_df=None, text=None, text_col='transcript', combine_every=1, minimum_sen_len = 2):

    if transcript_df == None:
        text = re.sub('\\n+'," ", text)
        sens = re.split("[.] ", text)
    else:
        sens = transcript_df[text_col]  
    sens_new = []
    count = 1
    lasting_sen = ""
    for sen in sens:
        lasting_sen += sen
        if count%combine_every == 0:
          sens_new.append(lasting_sen.strip())
          lasting_sen = ""
        count += 1
    if lasting_sen != "":
        sens_new.append(lasting_sen.strip())
    
    sens = [s for s in sens_new if len(s.split(" ")) >minimum_sen_len]
    sens_for_embed = []
    for s in sens:
      se = []
      for wd in s.split(" "):
        if wd.lower() not in stopwords.words('english'):
          se.append(wd)
      sens_for_embed.append(" ".join(se))

    return sens, sens_for_embed

class SummarisationAlgos():
    

    list_algos = [ 'BERTLexRank', 'IDFLexRank', 'AltAlgos']
    def __init__(self, algo_name='BERTLexRank', ):
      self.algo_name = algo_name
      
    def __call__(self, sens_text=None, txt=None, file=None, sentence_count=None):
      if self.algo_name == SummarisationAlgos.list_algos[0]:
        return self.BERTLexRank(sens_text)
      elif self.algo_name == SummarisationAlgos.list_algos[1]:
        return self.IDFLexRank(txt, file)
      elif self.algo_name == SummarisationAlgos.list_algos[2]:
        return self.AltAlgos(TEXT_FILE=file, LANGUAGE="english", SENTENCE_COUNT=sentence_count)


    class BERTLexRank():
        def __init__(self, sens_text=None):
        
          self.bertlexrank_matrix = None
          self.bertlexrank_repelling_sens = 0
          self.bertlexrank_attracting_sens = 0
          self.bertlexrank_rank_vector = None
          self.sens_text = sens_text
          self.bertlexrank_summary = None
          self.bertlexrank_summary_indices=None
          self.bertlexrank_mean_doc_vector = 1
          self.bertlexrank_iterative_rank_vector = None
          self.sens_text_overall = sens_text
          pass

        
        @staticmethod
        def cosine_similarity(sentence1, sentence2):

            
            numerator = np.dot(sentence1, sentence2)
            
            denominator = np.linalg.norm(sentence1)*np.linalg.norm(sentence2)

            return numerator/denominator
          

        @staticmethod
        def power_method(matrix, epsilon):

            transposed_matrix = matrix.T
            sentences_count = len(matrix)
            p_vector = np.array([1.0 / sentences_count] * sentences_count)
            lambda_val = 1.0
            tic = time.time()

            while lambda_val > epsilon:
                next_p = np.dot(transposed_matrix, p_vector)
            
                lambda_val = np.linalg.norm(np.subtract(next_p, p_vector))
                p_vector = next_p
                toc = time.time()
                if toc-tic >5:
                  break

            
            return p_vector

        def create_matrix(self, sentences, threshold):
            """
            Creates matrix of shape |sentences|×|sentences|.
            """
            # create matrix |sentences|×|sentences| filled with zeroes
            sentences_count = len(sentences)
            matrix = np.zeros((sentences_count, sentences_count))
            degrees = np.zeros((sentences_count, ))

            repelling_sens = 0
            attracting_sens = 0
            for row, sentence1 in enumerate(sentences):
                for col, sentence2 in enumerate(sentences):

                    if row != col or 1 :
                      if type(self.bertlexrank_mean_doc_vector) == int:
                       
                        matrix[row, col] = self.cosine_similarity(sentence1, sentence2)
                      else:
                        matrix[row, col] = min(0.8, self.cosine_similarity(self.bertlexrank_mean_doc_vector, sentence2))/(max(0.4, self.cosine_similarity(sentence1, sentence2)))

                      if matrix[row, col] >= threshold:
                          attracting_sens +=1
                          matrix[row, col] = 1.0
                          degrees[row] += 1
                      else:
                          repelling_sens += 1
                          matrix[row, col] = 0

            for row in range(sentences_count):
                for col in range(sentences_count):
                    if degrees[row] == 0:
                        degrees[row] = 1

                    matrix[row][col] = matrix[row][col] / degrees[row]
            
            
            matrix = self.reflect_matrix(matrix, sentences_count)
            self.bertlexrank_matrix = matrix
            self.bertlexrank_repelling_sens = int(sqrt(repelling_sens))
            self.bertlexrank_attracting_sens = int(sqrt(attracting_sens))
            return matrix

        def reflect_matrix(self, matrix, sentences_count):
          result = np.zeros((sentences_count, sentences_count))
          for x in range(0,sentences_count):
              for y in range(0,x+1):
                  result[x][y] = result[y][x] = matrix[x][y]
          return result

        def bert_vecs_lexrank(self, sen_vecs, sentence_count=-1, threshold_rank= -1, similarity_threshold=0.2, epsilon=1e-6, introduce_mean_doc_vector=None):

            if type(introduce_mean_doc_vector) != type(None):
                mean_doc_vector = introduce_mean_doc_vector
                self.bertlexrank_mean_doc_vector = introduce_mean_doc_vector

              
                if similarity_threshold == -1:
                  determine_cos_threshold = []
                  for i in range(len(sen_vecs)):
                    similaritiesd = cosine_similarity([sen_vecs[i]], sen_vecs)
                    similaritiesn = cosine_similarity([mean_doc_vector], sen_vecs)

                    num = np.array([min(0.8, similaritiesn[0][j]) for j in range(len(sen_vecs))])
                    den = np.array([max(0.6, similaritiesd[0][j]) for j in range(len(sen_vecs))])
                    determine_cos_threshold.append(np.median(num/(den)))
                  
                  determine_cos_threshold = np.mean(determine_cos_threshold)
                  similarity_threshold = round(determine_cos_threshold,4)
            else:

                if similarity_threshold == -1:
                  determine_cos_threshold = []
                  for i in range(len(sen_vecs)):
                    determine_cos_threshold.append(np.median(cosine_similarity([sen_vecs[i]], sen_vecs)))
                  
                  determine_cos_threshold = np.mean(determine_cos_threshold)
                  similarity_threshold = round(determine_cos_threshold,4)

           
            matrix = self.create_matrix(sen_vecs, similarity_threshold)
            ranks = self.power_method(matrix, epsilon)
            denominator = sum(list(ranks))
            ranks = ranks/denominator
            self.bertlexrank_rank_vector = ranks

            report = dict(zip([sen_index for sen_index in range(len(sen_vecs))], ranks))
            if threshold_rank == -1:
              threshold_rank = np.mean(ranks)
            elif threshold_rank == -2:
              threshold_rank = np.median(ranks)
            elif threshold_rank == -3:
              threshold_rank = np.percentile(ranks, 25)
            else:
              pass

            report_sorted = dict(sorted(report.items(), key=lambda item: item[1], reverse=True))
              

            summary_indices = []
            n = 0
            for index in report_sorted:
              if report_sorted[index] >= threshold_rank:
                summary_indices.append(index)
                n += 1
              if sentence_count != -1 and n == sentence_count:
                break

            summary_indices = sorted(summary_indices)
            self.bertlexrank_summary_indices = summary_indices
            summary = ""
              
            for index in summary_indices:
                summary += self.sens_text[index] + ". "


            self.bertlexrank_summary = summary
            return summary

        def iterative_lexrank_summary(self, sen_vecs, summary_window=10, sentence_count=-1, threshold_rank= -1, similarity_threshold=0.5, epsilon=1e-6, introduce_mean_doc_vector=None):
          iterations = ceil(len(sen_vecs)/summary_window)

          summary = ""
          ranks = np.empty((1,))
          for it in range(iterations):
            partial_sen_embeds = sen_vecs[it*summary_window:(it+1)*summary_window]
            self.sens_text = self.sens_text_overall[it*summary_window:(it+1)*summary_window]
            summary += self.bertsen_vecs_lexrank(partial_sen_embeds, sentence_count, threshold_rank, similarity_threshold, epsilon, introduce_mean_doc_vector)
            ranks = np.concatenate([ranks, self.lexrank_rank_vector], axis=0)
          
          self.bertlexrank_iterative_rank_vector = ranks[1:]
          self.bertlexrank_summary = summary
          self.bertlexrank_rank_vector = -1

          return summary

    class IDFLexRank():
      def __init__(self, txt=None, file=None):
        self.txt=txt
        self.file=file
        self.summary = None

      def idf_vecs_lexrank(self, sentence_count=10):
        if self.txt == None:
          parser = PlaintextParser.from_file(self.file, Tokenizer("english"))
        else:
          parser = PlaintextParser.from_string(self.txt, Tokenizer("english"))

        summarizer = LexRankSummarizer()
        summary = summarizer(parser.document,sentence_count) 
        summ = ""
        for sentence in summary:
          summ += str(sentence) + " "
        self.summary = summ
        return summ

    class AltAlgos():
      def __init__(self, TEXT_FILE=None, LANGUAGE="english", SENTENCE_COUNT=10):
        
        summ = sumySummarize(TEXT_FILE, LANGUAGE, SENTENCE_COUNT)
        self.summary = summ

def produce_ensembled_summary(summ_dict, sentence_count=1, original_sens=None):
  pool_summary = []
  for algo in summ_dict:
    sens = summ_dict[algo].split(". ")
    sens = [ele.strip() for ele in sens if ele != "" and ele != " "]
    pool_summary += sens

  sen_freq = dict()
  for sen in pool_summary:
    sen_freq[sen] = 0
  for sen in pool_summary:
    sen_freq[sen] += 1
  
  sen_freq_inorder = dict(sorted(sen_freq.items(), key=lambda item: item[1], reverse=True))
  sen_indices = dict()
  for k,v in enumerate(original_sens):
    sen_indices[v] = k

  summary_indices = []
  n = 0
  for sen in sen_freq_inorder.keys():
    if n > sentence_count:
      break
    summary_indices.append(sen_indices[sen])
    n += 1
  summary_indices = sorted(summary_indices)
  ensembled_summary = ""
  for indices in summary_indices:
    ensembled_summary += original_sens[indices] + ". "
  summ_dict['EnsembledSummary'] = ensembled_summary
  return summ_dict

def execute_summarisation(txt="Provide the text here", summary_ratio=0.4):
    try:
      os.mkdir('./Elite.ai-content-summarisations/')
    except OSError as error:
      pass
    time_stamp = str(time.time()//10**2)
    path_to_original = "./Elite.ai-content-summarisations/input_text"+"_"+time_stamp+".txt"
    input_fl = open(path_to_original, "w")
    input_fl.write(txt)
    clean_txt = re.sub('(https:.*\\n)|([0-9]+:[0-9]+)', '', txt)

    Senc = SentenceEncoders(model_name='paraphrase-mpnet-base-v2')
    enc = Senc.encoder
    original_sens, _ = SentenceEncoders.text_to_sens(text=clean_txt, combine_every=1)
    embed_sens = enc.encode(original_sens)

    mean_embed_sens = 0
    for i in range(len(embed_sens)):
      mean_embed_sens += embed_sens[i]
    mean_embed_sens = mean_embed_sens/len(embed_sens)

    embed_sens.shape, mean_embed_sens.shape
    #A higher similarity_threshold could sometimes introduce similar sentences or diverse sentences and it all depends on the majority of type of sentence present, for higher threshold rank set it to -2/1 for lower threshold rank set to -3 higher threshold rank is equivalent to
    #setting sentence_count lower
    #In the modified lexrank algorithm 0.4 similarity deviation of sentences in summary and similarity with mean_vector is clipped at 0.8

    summ_dict = dict()
    sentence_count = int(len(embed_sens)*summary_ratio)
    algon = SummarisationAlgos('AltAlgos')(file=path_to_original, sentence_count=sentence_count)
    summ_dict = algon.summary
    algo1 = SummarisationAlgos('BERTLexRank')(sens_text=original_sens)
    _ = algo1.bert_vecs_lexrank(embed_sens, sentence_count = sentence_count, threshold_rank = -1, similarity_threshold = 0.5, epsilon=1e-1, introduce_mean_doc_vector=mean_embed_sens)

    algo2 = SummarisationAlgos('IDFLexRank')(txt=clean_txt)
    _ = algo2.idf_vecs_lexrank(sentence_count=sentence_count)

    summ_dict['BERTLexRank'] = algo1.bertlexrank_summary
    summ_dict['IDFLexRank'] = algo2.summary

    summary = produce_ensembled_summary(summ_dict, sentence_count=sentence_count, original_sens=original_sens)['EnsembledSummary']

    path_to_summary = "./Elite.ai-content-summarisations/output_summary"+"_"+time_stamp+".txt"
    output_fl = open(path_to_summary, "w")
    output_fl.write(summary)

    return "~Done~"
#execute_summarisation("This is a test case. Let's see how it goes. The first sentence is important. The second one is not.", summary_ratio=0.8)

"""## BERT Embeddings with TensorFlow 2.0 Sources
With the new release of TensorFlow, this Notebook aims to show a simple use of the BERT model.
- See BERT on paper: https://arxiv.org/pdf/1810.04805.pdf
- See BERT on GitHub: https://github.com/google-research/bert
- See BERT on TensorHub: https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1
- See 'old' use of BERT for comparison: https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb
"""